{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings\n",
    "- BERT is language model by google.\n",
    "- We already have Word2Vec why we need BERT Embeddings.\n",
    "\n",
    "**Problem with Word2Vec**\n",
    "- Issue with Word2Vec is Fixed embeddings\n",
    "- Example:\n",
    "    - Sentence 1 - He didn't receive **fair** treatment\n",
    "    - Sentence 2 - Fun **fair** in New York this summer\n",
    "    - Here in both sentence \"fair\" has different contextual meaning but the vector generated in word2vec is same.\n",
    "- Thus we need a model which can generate a contextualized meaning of a word\n",
    "- Meaning, you look at the whole sentence and based on that you generate the number representation of word. BERT allow you do do the same.\n",
    "\n",
    "**BERT can generate contexualized embeddings**\n",
    "- Case 1:\n",
    "    - He didn't receive fair treatment -> fair -> [1, 0.9, 0.2, 1, 0.7]\n",
    "    - Tom deserves unbiased judgement -> unbaised -> [1, 0.8, 0.2, 1, 0.6]\n",
    "    - Here the worf \"fair\" has contextual meaning to \"unbaised\" thus vector is similar to \"unbaised\"\n",
    "- Case 2:\n",
    "    - Fun fair in New York city summer -> fair -> [0. 0.1, 0. 0.5, 0.4]\n",
    "    - Carnival was packed with fun activities -> carnival -> [0, 0.1, 0.1, 0.5, 0.3]\n",
    "    - Here the worf \"fair\" has contextual meaning to \"carnival\" thus vector is similar to \"carnival\"\n",
    "\n",
    "**BERT can generate embeddings for entire sentence**\n",
    "- can generate a single vector for complete sentence\n",
    "- vector dimension is 768\n",
    "\n",
    "**BERT Good Article**\n",
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "**How BERT was trained**\n",
    "- Trained on 2500 Million words from wikipedia and 800 Million words from books.\n",
    "- Trained using two Artificial Tasks and embeddings are generated as side effects of these tasks.\n",
    "    1) Mased Language Model - mased(removed) 15% words and train bert to fill the mased words.\n",
    "    2) Next Sentence Predection - predicting next sentence using some sentence.\n",
    "\n",
    "**BERT - Biderectional Encoder Representations from Transfotmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
    "encoder_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_mask', 'input_word_ids', 'input_type_ids'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = [\"nice movie indeed\", \"i love python programming\"]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_outputs', 'sequence_output', 'default', 'pooled_output'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.7917742 , -0.21411929,  0.497695  , ...,  0.2446516 ,\n",
       "        -0.47334483,  0.8175871 ],\n",
       "       [-0.91712314, -0.4793517 , -0.7865697 , ..., -0.61751723,\n",
       "        -0.7102687 ,  0.92184293]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results[\"pooled_output\"] # each sentence is converted to 768 dimension vector\n",
    "\n",
    "# you can use these vectors in you NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.0729205 ,  0.0856782 ,  0.14476842, ..., -0.09677088,\n",
       "          0.08722158,  0.07711128],\n",
       "        [ 0.17839359, -0.190061  ,  0.50349385, ..., -0.0586981 ,\n",
       "          0.3271711 , -0.15578535],\n",
       "        [ 0.18701442, -0.43388787, -0.48875144, ..., -0.15502739,\n",
       "          0.00145156, -0.24470964],\n",
       "        ...,\n",
       "        [ 0.12083052,  0.12884241,  0.4645356 , ...,  0.07375539,\n",
       "          0.17441928,  0.16522132],\n",
       "        [ 0.0796783 , -0.01190706,  0.5022544 , ...,  0.13777801,\n",
       "          0.21002187,  0.00624593],\n",
       "        [-0.07212718, -0.28303438,  0.5903338 , ...,  0.4755192 ,\n",
       "          0.16668445, -0.08920303]],\n",
       "\n",
       "       [[-0.07900569,  0.36335132, -0.21101536, ..., -0.17183745,\n",
       "          0.16299719,  0.67242664],\n",
       "        [ 0.27883506,  0.4371634 , -0.35764694, ..., -0.04463694,\n",
       "          0.383151  ,  0.5887983 ],\n",
       "        [ 1.2037675 ,  1.0727018 ,  0.4840878 , ...,  0.24921024,\n",
       "          0.407309  ,  0.4048182 ],\n",
       "        ...,\n",
       "        [ 0.08630021,  0.19353867,  0.47540027, ...,  0.18880153,\n",
       "         -0.06474115,  0.31318593],\n",
       "        [ 0.15887062,  0.28572658,  0.37340793, ...,  0.09309112,\n",
       "         -0.04969565,  0.3876109 ],\n",
       "        [-0.08079861, -0.09572858,  0.26809782, ...,  0.13979617,\n",
       "         -0.06315845,  0.27288306]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results[\"sequence_output\"] # each word of sentence is converted to vector of 768 dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_algo_py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
