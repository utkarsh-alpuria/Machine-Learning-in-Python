{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings\n",
    "- BERT is language model by google.\n",
    "- We already have Word2Vec why we need BERT Embeddings.\n",
    "\n",
    "**Problem with Word2Vec**\n",
    "- Issue with Word2Vec is Fixed embeddings\n",
    "- Example:\n",
    "    - Sentence 1 - He didn't receive **fair** treatment\n",
    "    - Sentence 2 - Fun **fair** in New York this summer\n",
    "    - Here in both sentence \"fair\" has different contextual meaning but the vector generated in word2vec is same.\n",
    "- Thus we need a model which can generate a contextualized meaning of a word\n",
    "- Meaning, you look at the whole sentence and based on that you generate the number representation of word. BERT allow you do do the same.\n",
    "\n",
    "**BERT can generate contexualized embeddings**\n",
    "- Case 1:\n",
    "    - He didn't receive fair treatment -> fair -> [1, 0.9, 0.2, 1, 0.7]\n",
    "    - Tom deserves unbiased judgement -> unbaised -> [1, 0.8, 0.2, 1, 0.6]\n",
    "    - Here the worf \"fair\" has contextual meaning to \"unbaised\" thus vector is similar to \"unbaised\"\n",
    "- Case 2:\n",
    "    - Fun fair in New York city summer -> fair -> [0. 0.1, 0. 0.5, 0.4]\n",
    "    - Carnival was packed with fun activities -> carnival -> [0, 0.1, 0.1, 0.5, 0.3]\n",
    "    - Here the worf \"fair\" has contextual meaning to \"carnival\" thus vector is similar to \"carnival\"\n",
    "\n",
    "**BERT can generate embeddings for entire sentence**\n",
    "- can generate a single vector for complete sentence\n",
    "- vector dimension is 768\n",
    "\n",
    "**BERT Good Article**\n",
    "https://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "**How BERT was trained**\n",
    "- Trained on 2500 Million words from wikipedia and 800 Million words from books.\n",
    "- Trained using two Artificial Tasks and embeddings are generated as side effects of these tasks.\n",
    "    1) Mased Language Model - mased(removed) 15% words and train bert to fill the mased words.\n",
    "    2) Next Sentence Predection - predicting next sentence using some sentence.\n",
    "\n",
    "**BERT - Biderectional Encoder Representations from Transfotmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
    "encoder_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayers(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = [\"nice movie indeed\", \"i love python programming\"]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(encoder_url)\n",
    "\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results[\"pooled_output\"] # each sentence is converted to 768 dimension vector\n",
    "\n",
    "# you can use these vectors in you NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results[\"sequence_output\"] # each word of sentence is converted to vector of 768 dimension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml_algo_py_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
